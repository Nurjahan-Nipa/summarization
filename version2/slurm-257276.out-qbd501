Sat Apr 19 15:46:44 CDT 2025
Starting LLaMA 3.3 70B inference job...
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 CDT
 1  0      0 514851040 1105492 7427524    0    0     0     0    0    0  1  1 99  0  0 2025-04-19 15:46:44
index, name, utilization.gpu [%], memory.used [MiB], memory.total [MiB]
0, NVIDIA A100 80GB PCIe, 0 %, 4 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 0 %, 4 MiB, 81920 MiB
INFO 04-19 15:46:57 [__init__.py:239] Automatically detected platform cuda.
INFO 04-19 15:47:10 [config.py:585] This model supports multiple tasks: {'score', 'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
INFO 04-19 15:47:10 [config.py:1519] Defaulting to use mp for distributed inference
INFO 04-19 15:47:10 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 04-19 15:47:10 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 04-19 15:47:12 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.3-70B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 04-19 15:47:12 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-19 15:47:12 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_1c3244ce'), local_subscribe_addr='ipc:///tmp/8c3b3c4a-5ee7-42e1-ba45-71452c258afd', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 04-19 15:47:13 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14831b37ca50>
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:47:13 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5f103d7d'), local_subscribe_addr='ipc:///tmp/f6002ea9-09a3-4b76-ab8b-df1b3bc662f7', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 04-19 15:47:13 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14831b37c9d0>
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:47:13 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1bc8b62f'), local_subscribe_addr='ipc:///tmp/d9c2cc49-2756-4ef9-8da2-06631af12f43', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:47:14 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:47:14 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:47:14 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:47:14 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:47:15 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nurjahan/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:47:15 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nurjahan/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:47:15 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_2a6aebf1'), local_subscribe_addr='ipc:///tmp/c0d22c0f-4195-474e-8778-391b3e3151ec', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:47:15 [parallel_state.py:954] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:47:15 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:47:15 [parallel_state.py:954] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:47:15 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:47:15 [gpu_model_runner.py:1174] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:47:15 [gpu_model_runner.py:1174] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m WARNING 04-19 15:47:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m WARNING 04-19 15:47:16 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:47:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:47:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:48:12 [loader.py:447] Loading weights took 55.81 seconds
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:48:12 [loader.py:447] Loading weights took 55.66 seconds
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m WARNING 04-19 15:48:13 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m WARNING 04-19 15:48:13 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=1 pid=2379364)[0;0m INFO 04-19 15:48:14 [gpu_model_runner.py:1186] Model loading took 33.8772 GB and 58.991259 seconds
[1;36m(VllmWorker rank=0 pid=2379353)[0;0m INFO 04-19 15:48:14 [gpu_model_runner.py:1186] Model loading took 33.8772 GB and 58.998242 seconds
INFO 04-19 15:48:30 [kv_cache_utils.py:566] GPU KV cache size: 185,328 tokens
INFO 04-19 15:48:30 [kv_cache_utils.py:569] Maximum concurrency for 2,048 tokens per request: 90.49x
INFO 04-19 15:48:30 [kv_cache_utils.py:566] GPU KV cache size: 185,328 tokens
INFO 04-19 15:48:30 [kv_cache_utils.py:569] Maximum concurrency for 2,048 tokens per request: 90.49x
INFO 04-19 15:48:30 [core.py:151] init engine (profile, create kv cache, warmup model) took 15.78 seconds
 3  0      0 489567520 1105492 26971488    0    0     9    39 25619 33394  3  1 96  0  0 2025-04-19 15:51:44
0, NVIDIA A100 80GB PCIe, 91 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 87 %, 67853 MiB, 81920 MiB
 3  0      0 489418816 1105492 26977620    0    0     0    28 4663 2442  5  0 95  0  0 2025-04-19 15:56:44
0, NVIDIA A100 80GB PCIe, 76 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 90 %, 67853 MiB, 81920 MiB
 3  0      0 489303200 1105492 26983812    0    0     0    28 4623 2106  5  0 95  0  0 2025-04-19 16:01:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
 3  0      0 489163616 1105492 26999340    0    0     0    26 4571 2274  4  0 95  0  0 2025-04-19 16:06:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
 3  0      0 489110112 1105492 27005404    0    0     0    27 4558 2231  5  0 95  0  0 2025-04-19 16:11:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 95 %, 67853 MiB, 81920 MiB
 4  0      0 489051424 1105492 27011488    0    0     0    26 4559 2229  5  0 95  0  0 2025-04-19 16:16:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
 3  0      0 488880448 1105492 27017684    0    0     0    27 4606 2308  4  0 95  0  0 2025-04-19 16:21:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
 3  0      0 488818752 1105492 26925424    0    0     0    48 4690 2226  5  0 95  0  0 2025-04-19 16:26:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
 4  0      0 488694144 1105492 26931736    0    0     0    28 4792 2333  5  0 95  0  0 2025-04-19 16:31:44
0, NVIDIA A100 80GB PCIe, 97 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 95 %, 67853 MiB, 81920 MiB
 3  0      0 488598496 1105492 26937876    0    0     0    26 4627 2196  4  0 95  0  0 2025-04-19 16:36:44
0, NVIDIA A100 80GB PCIe, 97 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
 3  0      0 488447808 1105492 26943924    0    0     0    26 4599 2264  5  0 95  0  0 2025-04-19 16:41:44
0, NVIDIA A100 80GB PCIe, 97 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
 3  0      0 488338656 1105492 26949932    0    0     0    26 4520 2267  4  0 95  0  0 2025-04-19 16:46:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
 3  0      0 488219456 1105492 26955960    0    0     0    26 4494 2203  4  0 95  0  0 2025-04-19 16:51:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 97 %, 67853 MiB, 81920 MiB
 4  0      0 509036544 1105492 7691940    0    0     0    28 4744 2593  4  0 95  0  0 2025-04-19 16:56:44
0, NVIDIA A100 80GB PCIe, 85 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 78 %, 67853 MiB, 81920 MiB
 3  0      0 509036512 1105492 7698152    0    0     0    28 4600 2272  4  0 95  0  0 2025-04-19 17:01:44
0, NVIDIA A100 80GB PCIe, 56 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 30 %, 67853 MiB, 81920 MiB
 3  0      0 509029728 1105492 7704200    0    0     0    26 4555 2222  4  0 95  0  0 2025-04-19 17:06:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 96 %, 67853 MiB, 81920 MiB
 3  0      0 509023872 1105492 7710276    0    0     0    27 4495 2192  4  0 95  0  0 2025-04-19 17:11:44
0, NVIDIA A100 80GB PCIe, 96 %, 69070 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 97 %, 67853 MiB, 81920 MiB
 3  0      0 509016480 1105492 7717112    0    0     0    26 4479 2206  4  0 95  0  0 2025-04-19 17:16:44

📊 ROUGE Scores:
rouge1: 0.5590
rouge2: 0.5090
rougeL: 0.5433

📊 BERTScore:
Precision: 0.8956
Recall:    0.8706
F1:        0.8822

📊 BLEU Score:
BLEU: 0.3889

🕒 Inference Time: 5425.47 seconds
💾 Max memory allocated: 0.68 GB
💾 Max memory reserved:  0.69 GB
Job completed.
Sat Apr 19 17:19:41 CDT 2025
