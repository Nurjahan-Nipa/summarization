Mon Apr 21 00:55:59 CDT 2025
Starting LLaMA 3.1 8B fp8 llamav2  with few shot
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 CDT
 1  0 377392 514138368 1077224 8246884    0    0     0     1    0    0  0  1 99  0  0 2025-04-21 00:55:59
index, name, utilization.gpu [%], memory.used [MiB], memory.total [MiB]
0, NVIDIA A100 80GB PCIe, 0 %, 4 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 0 %, 4 MiB, 81920 MiB
INFO 04-21 00:56:08 [__init__.py:239] Automatically detected platform cuda.
Loading models...
INFO 04-21 00:56:20 [config.py:585] This model supports multiple tasks: {'reward', 'score', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 04-21 00:56:20 [config.py:1519] Defaulting to use mp for distributed inference
INFO 04-21 00:56:20 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 04-21 00:56:20 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 04-21 00:56:22 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 04-21 00:56:22 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-21 00:56:22 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_6a0ad1b1'), local_subscribe_addr='ipc:///tmp/d252c41b-fcd4-418b-9dc9-0f558b51e883', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 04-21 00:56:22 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14eb750ca550>
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:22 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8b193f2b'), local_subscribe_addr='ipc:///tmp/935f7c2f-35c5-48f2-beb9-938f4bff132e', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 04-21 00:56:23 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14eb74fa32d0>
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:23 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b59ff782'), local_subscribe_addr='ipc:///tmp/344c17fa-8594-4b12-81dc-719ec57b6cc0', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:23 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:23 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:23 [utils.py:931] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:23 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:25 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nurjahan/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:25 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/nurjahan/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:25 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_13d105ce'), local_subscribe_addr='ipc:///tmp/d12ea611-3bbb-4b85-89b0-7b1aec744bd7', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:25 [parallel_state.py:954] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:25 [parallel_state.py:954] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:25 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:25 [cuda.py:220] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:25 [gpu_model_runner.py:1174] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:25 [gpu_model_runner.py:1174] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
[1;36m(VllmWorker rank=1 pid=795459)[0;0m WARNING 04-21 00:56:25 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=795448)[0;0m WARNING 04-21 00:56:25 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:32 [loader.py:447] Loading weights took 6.11 seconds
[1;36m(VllmWorker rank=0 pid=795448)[0;0m WARNING 04-21 00:56:32 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:32 [loader.py:447] Loading weights took 5.70 seconds
[1;36m(VllmWorker rank=1 pid=795459)[0;0m WARNING 04-21 00:56:32 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=1 pid=795459)[0;0m INFO 04-21 00:56:32 [gpu_model_runner.py:1186] Model loading took 4.2646 GB and 6.836859 seconds
[1;36m(VllmWorker rank=0 pid=795448)[0;0m INFO 04-21 00:56:32 [gpu_model_runner.py:1186] Model loading took 4.2646 GB and 6.809858 seconds
INFO 04-21 00:56:36 [kv_cache_utils.py:566] GPU KV cache size: 922,704 tokens
INFO 04-21 00:56:36 [kv_cache_utils.py:569] Maximum concurrency for 2,048 tokens per request: 450.54x
INFO 04-21 00:56:36 [kv_cache_utils.py:566] GPU KV cache size: 922,704 tokens
INFO 04-21 00:56:36 [kv_cache_utils.py:569] Maximum concurrency for 2,048 tokens per request: 450.54x
INFO 04-21 00:56:36 [core.py:151] init engine (profile, create kv cache, warmup model) took 3.83 seconds
Loading dataset...
Starting inference...
 4  0 377392 493907008 1077228 23241116    0    0     0    29 7892 18734  4  0 96  0  0 2025-04-21 01:00:59
0, NVIDIA A100 80GB PCIe, 37 %, 70550 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 83 %, 69331 MiB, 81920 MiB
 3  0 377392 493785888 1077228 23247120    0    0     0    26 4558 2621  5  0 95  0  0 2025-04-21 01:05:59
0, NVIDIA A100 80GB PCIe, 48 %, 70550 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 75 %, 69331 MiB, 81920 MiB
 3  0 377392 493704480 1077228 23253160    0    0     0    27 4659 2871  5  0 95  0  0 2025-04-21 01:10:59
0, NVIDIA A100 80GB PCIe, 43 %, 70550 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 85 %, 69331 MiB, 81920 MiB
 3  0 377392 493579776 1077232 23259232    0    0     0    25 4638 2741  5  0 95  0  0 2025-04-21 01:15:59
0, NVIDIA A100 80GB PCIe, 43 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 83 %, 69331 MiB, 81920 MiB
 3  0 377392 493503168 1077236 23265304    0    0     0    27 4539 2381  5  0 95  0  0 2025-04-21 01:20:59
0, NVIDIA A100 80GB PCIe, 43 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 84 %, 69331 MiB, 81920 MiB
 3  0 377392 493457696 1077236 23271344    0    0     0    28 4663 2523  5  0 95  0  0 2025-04-21 01:25:59
0, NVIDIA A100 80GB PCIe, 43 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 85 %, 69331 MiB, 81920 MiB
 3  0 377392 493347136 1077236 23275312    0    0     0    27 4576 2792  5  0 95  0  0 2025-04-21 01:30:59
0, NVIDIA A100 80GB PCIe, 43 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 85 %, 69331 MiB, 81920 MiB
 3  0 377392 497806368 1077236 19003276    0    0     0    26 4611 2516  5  0 95  0  0 2025-04-21 01:35:59
0, NVIDIA A100 80GB PCIe, 43 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 82 %, 69331 MiB, 81920 MiB
 3  0 377392 497845664 1077236 18779016    0    0     0    26 4507 2701  5  0 95  0  0 2025-04-21 01:40:59
0, NVIDIA A100 80GB PCIe, 39 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 50 %, 69331 MiB, 81920 MiB
 3  0 377392 498077696 1077240 18456916    0    0     0    25 4575 2587  5  0 95  0  0 2025-04-21 01:45:59
0, NVIDIA A100 80GB PCIe, 48 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 54 %, 69331 MiB, 81920 MiB
 3  0 377392 497935648 1077244 18462992    0    0     0    27 4728 2744  5  0 95  0  0 2025-04-21 01:50:59
0, NVIDIA A100 80GB PCIe, 45 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 75 %, 69331 MiB, 81920 MiB
 3  0 377392 498061312 1077256 18271544    0    0     0    42 4669 2516  5  0 95  0  0 2025-04-21 01:55:59
0, NVIDIA A100 80GB PCIe, 44 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 81 %, 69331 MiB, 81920 MiB
 3  0 377392 497987008 1077260 18253468    0    0     0    27 4631 2661  5  0 95  0  0 2025-04-21 02:00:59
0, NVIDIA A100 80GB PCIe, 41 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 46 %, 69331 MiB, 81920 MiB
 3  0 377392 510904960 1077264 6316628    0    0     0    27 4729 2948  4  0 95  0  0 2025-04-21 02:05:59
0, NVIDIA A100 80GB PCIe, 44 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 85 %, 69331 MiB, 81920 MiB
 3  0 377392 510893472 1077268 6322672    0    0     0    27 4620 2543  5  0 95  0  0 2025-04-21 02:10:59
0, NVIDIA A100 80GB PCIe, 44 %, 70552 MiB, 81920 MiB
1, NVIDIA A100 80GB PCIe, 84 %, 69331 MiB, 81920 MiB

=== Final Results ===
?? Time: 4703.49 sec | ?? Mem Alloc: 3.10 GB | Reserved: 3.95 GB
?? ROUGE-1 F1: 0.5692
?? ROUGE-2 F1: 0.5023
?? ROUGE-L F1: 0.5214
?? BERTScore:
  Precision: 0.8721
  Recall:    0.8713
  F1:        0.8712
?? BLEU: 0.4191
Done!
